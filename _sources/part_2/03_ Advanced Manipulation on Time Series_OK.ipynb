{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invisible-pursuit",
   "metadata": {},
   "source": [
    "# Chapter 3: Advanced Manipulation on Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-delight",
   "metadata": {},
   "source": [
    "Time series data captures sequential information over consistent time intervals. However, for an effective analysis and modeling, further preprocessing techniques and special type of operations are often applied. Some of the insights one can gain from those operations include  stabilizing the mean of a series,  smoothening the dynamics of the series, reducing noise and highlighting underlying trends just to name a few. \n",
    "\n",
    "Practical use cases for these operations are critical when working on forecasting stock prices for instance, analyzing weather patterns, predicting customer demand, and detecting anomalies in sensor data. By mastering these operations, you can gain valuable insights into temporal patterns, identify meaningful trends, and make informed decisions based on historical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-rocket",
   "metadata": {},
   "source": [
    "**Learning Objectives:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-specific",
   "metadata": {},
   "source": [
    "In this section, you will gain a comprehensive understanding of time series preprocessing techniques including:\n",
    "\n",
    "1. Stabilizing the mean of a series through Differentiation operations\n",
    "\n",
    "2. Smoothening the series or reducing the noise through rolling mean or moving average\n",
    "\n",
    "3. Enriching signal clarity or isolating cyclic patterns within a series through filtering techniques\n",
    "\n",
    "3. Reducing the computational complexity through downsampling mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-publication",
   "metadata": {},
   "source": [
    "## Components of Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-exposure",
   "metadata": {},
   "source": [
    "Time series analysis provides a body of techniques to better understand a dataset. Perhaps the\n",
    "most useful of these is the decomposition of a time series into 4 constituent parts:\n",
    "\n",
    "* **Level.** The baseline value for the series if it were a straight line.\n",
    "\n",
    "* **Trend.** The optional and often linear increasing or decreasing behavior of the series over time.\n",
    "\n",
    "* **Seasonality.** The optional repeating patterns or cycles of behavior over time.\n",
    "\n",
    "* **Noise.** The optional variability in the observations that cannot be explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-tanzania",
   "metadata": {},
   "source": [
    "These constituent components can be thought to combine in some way to provide the\n",
    "observed time series. Assumptions can be made about these components both in behavior\n",
    "and in how they are combined, which allows them to be modeled using traditional statistical\n",
    "methods. These components may also be the most effective way to make predictions about\n",
    "future values, but not always. In cases where these classical methods do not result in effective\n",
    "performance, these components may still be useful concepts, and even input to alternate methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-mozambique",
   "metadata": {},
   "source": [
    ">#### <font color=#800080> Example: </font> <a class=\"anchor\" id=\"Task-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-session",
   "metadata": {},
   "source": [
    "Arusha is a city up north in Tanzania, widely recognized for its several distinctive attributes and significant contributions to both Tanzania and East Africa. Specifically, it holds the distinguished reputation of being Tanzania's safari capital city and a popular stopover for adventurers who are preparing for a Kilimanjaro expedition. Given the impact of climate change, The Tanzania Tourist Board, which is the national tourism organization, seeks your expertise in examining climate patterns spanning the past three months, from 01 May 2023 to 31st July 2023. This assessment will inform their promotion of tourism activities while considering weather conditions.\n",
    "\n",
    "\n",
    "1. What is Arusha sadly known for? Hint: ...\"Rwanda, August 1993\".\n",
    "\n",
    "\n",
    "The weather data has been provided by the Tanzania Meteorological Authority(TMA). On an hourly basis, several variables have been measured:\n",
    "\n",
    "    * The Wind Direction at 50 Meters (Degrees)  \n",
    "    * The Wind Speed at 50 Meters (m/s) \n",
    "    * The Temperature at 2 Meters (Dregrees Celsius)\n",
    "    * The Precipitation Corrected (mm/hour)  \n",
    "    * Specific Humidity at 2 Meters (g/kg)\n",
    "\n",
    "The data has been shared with you as a csv file called `arusha_hourlyseries.csv` .\n",
    "\n",
    "2. Load it and tell us what you observe just from looking at the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you open the data within a spreadsheet, you will understand why we had to include the skiprows parameter.\n",
    "arusha_data =  pd.read_csv('data/arusha_hourlyseries.csv' , skiprows=13)\n",
    "arusha_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-trustee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arusha_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-german",
   "metadata": {},
   "source": [
    "We noticed that the **YEAR**, **MONTH** and **DAY** column are all displayed separately. We might to re-assemble them into a date column with the correspond data type. Prior to do that, let's rename those columbs appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data.columns = ['year' , 'month' , 'day' , 'hour' ,\n",
    "                       'WD50M', 'T2M', 'WS50M', 'PRECTOTCORR','QV2M' ]\n",
    "arusha_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data['Date'] = pd.to_datetime(arusha_data[[\"year\", \"month\", \"day\"]])\n",
    "arusha_data =  arusha_data[['Date']+ list(arusha_data.columns[:-1])]\n",
    "arusha_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-occurrence",
   "metadata": {},
   "source": [
    "The same applies to the **hours** column. But here, we convert the integer representation into an hour column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data['hour'] = pd.to_datetime(arusha_data['hour'], unit='h').dt.time\n",
    "arusha_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-complement",
   "metadata": {},
   "source": [
    "Let's look at the tendancy for each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = ['r' , 'g' , 'b' , 'k' , 'cyan']\n",
    "for var , col  in zip(arusha_data.columns[5:] ,colors):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.plot(arusha_data[var] ,  color=col)\n",
    "    plt.title(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-background",
   "metadata": {},
   "source": [
    "To get the flavour of the series, we might also want to investigate the start date and the end data of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[\"Date\"].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-representative",
   "metadata": {},
   "source": [
    "We could see that the data collection started on **01st May 2023** and ended on **01st August 2023** at midnight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-robinson",
   "metadata": {},
   "source": [
    "### Differencing time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-bowling",
   "metadata": {},
   "source": [
    "\n",
    "Differencing is a method of transforming a time series dataset. It can be used to remove the series dependence on time, so-called temporal dependence. This includes structures like trends and seasonality. Taking the difference between consecutive observations is called a lag-1 difference. The lag difference can be adjusted to suit the specific temporal structure. you can use the `.diff()` method to perform that.\n",
    "\n",
    "Let's say we are interested in knowing the temperature difference between two consecutive days within the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data['T2M'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[\"Temp_diff\"] = arusha_data[\"T2M\"].diff()\n",
    "arusha_data[['Date' , 'T2M', 'Temp_diff']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-victoria",
   "metadata": {},
   "source": [
    "As you could notice, the first entry of the `Temperature difference` column has `NaN` as a placeholder as it refers the starting date, which means there is no value to differentiate against. \n",
    "\n",
    "The differentiation becomes useful when we are concerned about removing the linear trend from the dynamics of the series. Let's see how the temperature difference looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(arusha_data['Temp_diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-anniversary",
   "metadata": {},
   "source": [
    "One could also change the lag difference in order to adress a specific temporal structure. By passing an integer parameter to the `.diff()` method, we can compute the difference between two timely distant observations of the variable. For instance, the temperature difference every two hours, will be computed the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[\"T2M\"].diff(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-semiconductor",
   "metadata": {},
   "source": [
    "### Cumulating  time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-polymer",
   "metadata": {},
   "source": [
    "As opposed to differencing, we might be concerned with tracking the accumulated total of a variable over time, in order to understand the growth patterns, comparing year-to-date figures or observing the accumulation rate of a particular metric. \n",
    "\n",
    "For the case in hand, it won't make a lot of sense to compute the accumulated temperature over hours/days, as temperature is an intensive physical property of matter that is not additive. However, computing the accumulated precipitation over some time period could tell us about the pluviometry of that location or the total amount of rainfall within a specific time frame within that location. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[\"Prec_cum\"] = arusha_data[\"PRECTOTCORR\"].cumsum()\n",
    "arusha_data[['Date' , 'PRECTOTCORR', 'Prec_cum']].head(10)\n",
    "arusha_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-israel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(arusha_data['Prec_cum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_data[arusha_data['month'] == 5].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-liability",
   "metadata": {},
   "source": [
    "This could in turn answer questions related to the monthly total amount of rainfall in arusha and we could also observe its dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_Tot_rfall = arusha_data.groupby('month')['Prec_cum'].last()\n",
    "monthly_Tot_rfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-columbus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(['May' , 'June', 'July' , 'August'] , monthly_Tot_rfall ,'g')\n",
    "plt.ylabel('precipitation')\n",
    "plt.xlabel('month')\n",
    "# plt.title('Monthly Total amount of rainfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-education",
   "metadata": {},
   "source": [
    "### Rolling Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-vehicle",
   "metadata": {},
   "source": [
    "While the daily or hourly temperature fluctuations can be quite erratic due to various transient factors, we might be interested in identifying any longer-term trends or anomalies that might suggest broader climatic shifts. The rolling mean (often referred to as the moving average) is a powerful tool to tackle that issue.\n",
    "\n",
    "By applying a rolling mean with a fixed window size, we can smooth out the day-to-day fluctuations and clearly see monthly patterns. For instance, let's look at the moving average given a windown of fixed size 12, in order to tell how the data behaves on a 12hours basis.\n",
    "\n",
    "We can use the `.rolling()` method, which takes a parameter of the number of values to consider in the rolling window. In the example below, we take the mean of six values, in order to have the moving average every six consecutive hours of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-brief",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arusha_data['rolling_6h_mean'] = arusha_data['T2M'].rolling( window= 6).mean()\n",
    "arusha_data[['Date' , 'T2M' , 'rolling_6h_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-founder",
   "metadata": {},
   "source": [
    "### Resampling the Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-honolulu",
   "metadata": {},
   "source": [
    "There are several scenarios where one might need to resample the time series, which is a fundamental step in time series analysis. Resampling time series data refers to the process of changing the time-frequency or granularity of the data points. This can be either **an increase in frequency (upsampling)** or **a reduction (downsampling)**. By adapting the frequency, analysts can align datasets with different intervals for consistent comparisons or analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-grain",
   "metadata": {},
   "source": [
    "### Downsampling the Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-genre",
   "metadata": {},
   "source": [
    "Downsampling the series, which involves reducing the data's granularity, is particularly useful in improving computational efficiency, reducing noise and providing a higher-level view of patterns or trends.\n",
    "\n",
    "1. Improving Computation Efficiency:\n",
    "\n",
    "For very large datasets, computation can become resource-intensive and time-consuming. Downsampling can make data processing and modeling more manageable.\n",
    "\n",
    "2. Noise Reduction:\n",
    "\n",
    "High-frequency data can sometimes introduce a lot of noise. Downsampling, when combined with aggregation (like taking the mean), can help in smoothening the data and removing short-term fluctuations or noise, revealing longer-term trends or cycles.\n",
    "\n",
    "3. Visualizations:\n",
    "\n",
    "Too many data points can make visualizations cluttered and less informative. Downsampling can make plots and charts clearer and easier to understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-relaxation",
   "metadata": {},
   "source": [
    "A practical example would be to convert hourly data to daily data or daily data to monthtly or to any frequency that is dictated by the problem. For instance, let's look at the distribution of the wind speed on a day to day basis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "for day   in range(12):\n",
    "        ax =  plt.subplot(4,3, day+1)\n",
    "        plt.plot(arusha_data['WS50M'].values[24*day:24*(day+1)])\n",
    "        #plt.xlabel('hour')\n",
    "        plt.ylabel('wind speed')\n",
    "        #plt.title('day ' +str(day+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-health",
   "metadata": {},
   "source": [
    "Despite the wind's overall daily fluctuations, there may not be a significant change within two consecutive hours. This suggests that if we encounter one of the aforementioned situations, we could downsample the wind speed signal by recording it every two hours. Consequently, there would be twelve data points in a day instead of twenty-four.\n",
    "\n",
    "It is achieving by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_per2h = arusha_data[::2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-infection",
   "metadata": {},
   "source": [
    "Below is the resulting daily plots of wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "for day   in range(12):\n",
    "        ax =  plt.subplot(4,3, day+1)\n",
    "        plt.plot(arusha_per2h['WS50M'].values[12*day:12*(day+1)])\n",
    "        #plt.xlabel('hour')\n",
    "        plt.ylabel('wind speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-passion",
   "metadata": {},
   "source": [
    "Another downsampling strategy involves reducing the frequency of data points, by computing an aggregate of a certain time interval. For instance, in case of low variation of the signals, we might consider only working with daily averages, which means that the 24 data points available within a day are therefore represented by their average value. To work that out effectively with dataframes, we would have to set the date column as dataframe index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_indexed= arusha_data.set_index([\"Date\"])\n",
    "arusha_indexed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-advocacy",
   "metadata": {},
   "source": [
    "And then, we can take the daily mean of the observations and see the corresponding plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_dsp1 = pd.DataFrame(arusha_indexed.resample(\"D\")['T2M','WS50M' ].mean())#.reset_index()\n",
    "arusha_dsp1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-prize",
   "metadata": {},
   "source": [
    "Below we have the daily average of temperature over the course of a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "days =  ['day' + str(i+1) for i in range(30)]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(days, arusha_dsp1['T2M'].head(30))\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('temperature')\n",
    "locator = matplotlib.ticker.MultipleLocator(4)\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-toronto",
   "metadata": {},
   "source": [
    "Below we have the daily sum of precipitation over the course of a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_dsp2 = pd.DataFrame(arusha_indexed.resample(\"D\")['PRECTOTCORR','QV2M' ].sum())#.reset_index()\n",
    "arusha_dsp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "days =  ['day' + str(i+1) for i in range(30)]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(days, arusha_dsp2['PRECTOTCORR'].head(30))\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('precipitation')\n",
    "locator = matplotlib.ticker.MultipleLocator(4)\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-crash",
   "metadata": {},
   "source": [
    "### Upsampling the Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-space",
   "metadata": {},
   "source": [
    "On the other hand, upsampling increases the granularity, which can be beneficial for filling gaps in data or preparing data for certain models or applications that require a specific frequency. When resampling, the choice of method for filling or aggregating values—such as linear interpolation, mean, or sum—is crucial to ensure meaningful and accurate representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-davis",
   "metadata": {},
   "source": [
    "\n",
    "Upsampling is when the frequency of samples is increased (e.g., months to days).\n",
    "Again, you can use the\n",
    "`.resample()`\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-confidentiality",
   "metadata": {},
   "source": [
    "Let's consider the downsamples, we could choose the downsampled temperature data that we performed earlier and try and resample it using the linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "arusha_dsp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-auckland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arusha_intdaily = arusha_dsp1.resample(\"H\").interpolate(method = \"linear\")\n",
    "arusha_intdaily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "for day   in range(12):\n",
    "        ax =  plt.subplot(4,3, day+1)\n",
    "        plt.plot(arusha_data['WS50M'].values[24*day:24*(day+1)] ,label = 'true data')\n",
    "        plt.plot(arusha_intdaily['WS50M'].values[24*day:24*(day+1)],label = 'intpd data')\n",
    "        plt.ylabel('wind speed')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-dialogue",
   "metadata": {},
   "source": [
    "We can see that the linear interpolation is not the best representation of our observations. One should probably think of an polynomial interpolation and choose an convenient degree for the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-balloon",
   "metadata": {},
   "source": [
    ">#### <font color=#800080>Task 4:</font> <a class=\"anchor\" id=\"Task-1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-elements",
   "metadata": {},
   "source": [
    "**SahelPower Co.** is an energy company located in Moundou,  the second largest city in Chad. They primarily relies on wind farms to generate electricity. The company understands that electricity demand is influenced by various factors, including ambient temperature as in hot weather, households tend to consume more energy.\n",
    "\n",
    "However, their wind power generation platform is ran by a subisidiary start-up specialized in wind powerplants constructions called SaoWind. SaoWind provide wind-related data, and SahelPower Co. has the technology to convert it into wind energy for electricity usage.\n",
    "\n",
    "Their overall goal is to optimize their electricity production, manage demand-supply gaps, and improve overall efficiency through analyzing closely the  datasets they have received from both SNE (Societe Nationale d'Electricite du Tchad) and SaoWind (wind speed and direction).\n",
    "\n",
    "Here are a few additional information regarding the data files they have received:\n",
    "\n",
    "* SaoWind:  \n",
    "\n",
    "    1. Variables: Wind speed at 50M  , Wind Direction at 50M\n",
    "    2. Resolution: hourly observations\n",
    "    3. Date range: 01 Jan 2004 at 10AM  - 26 Feb 2004  at 9AM\n",
    "    4. file name:  `saowind_data.csv`\n",
    "    \n",
    "* SNE \n",
    "\n",
    "    1. Variables: Electricity demand, Ambiant Temperature\n",
    "    2. Resolution: hourly observations\n",
    "    3. Date range: 01 Jan 2004 at 00AM  - 26 Feb 2004  at 11PM\n",
    "    4. file name:  `snelec_data.csv`\n",
    "\n",
    "\n",
    "1. Wind is often considered as one of the cleanest form of renewable energy? What are the pros and cons  of it?\n",
    "\n",
    "2. You have noticed that data came from two different sheets. Merge them and report on the different steps you used to achieve that.\n",
    "\n",
    "3. Plot the temperature and demand  seperately and comment on the plots\n",
    "\n",
    "\n",
    "4. How do the average electricity demand and the ambient temperature vary across different weeks? Commenton  your findings.\n",
    "\n",
    "5.  On days when the ambient temperature was above 25°C, was the electricity demand significantly higher than on days when it was below 25°C?\n",
    "\n",
    "\n",
    "6. Plot the temperature differences between Saturday withing the observations.\n",
    "\n",
    "\n",
    "7. How does the average electricity generation during daytime hours (e.g., 6 am to 6 pm) compare to the average generation during nighttime hours (e.g., 6 pm to 6 am)? \n",
    "\n",
    "\n",
    "8. The WD50M measure the direction of the wind towards the North Direction. However, for  wind power calculation, due to the singularity around  0 and 360 degrees, it was decided to convert the wind speed and direction into wind vector (x and y components) using the formula below:\n",
    "\n",
    "$w_x = v \\cdot \\cos(\\phi)$ \n",
    "\n",
    "$ w_y = v  \\cdot \\sin(\\phi)$\n",
    "\n",
    "\n",
    "where  $v$ is the wind velocity and $\\phi$ is the wind angle. Compute the x and y computer components of the wind vector and plot the corresponding graphs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
